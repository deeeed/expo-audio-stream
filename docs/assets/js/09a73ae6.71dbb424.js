"use strict";(self.webpackChunkdocumentation_site=self.webpackChunkdocumentation_site||[]).push([[4767],{8029:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"api-reference/audio-features/audio-analysis-example","title":"Audio Analysis Example","description":"This example demonstrates how to use the audio analysis features in a real-world application. We\'ll create a simple component that records audio, analyzes it, and displays the results.","source":"@site/docs/api-reference/audio-features/audio-analysis-example.md","sourceDirName":"api-reference/audio-features","slug":"/api-reference/audio-features/audio-analysis-example","permalink":"/expo-audio-stream/docs/api-reference/audio-features/audio-analysis-example","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"audio-analysis-example","title":"Audio Analysis Example","sidebar_label":"Usage Example"},"sidebar":"tutorialSidebar","previous":{"title":"extractAudioAnalysis","permalink":"/expo-audio-stream/docs/api-reference/audio-features/extract-audio-analysis"},"next":{"title":"extractMelSpectrogram","permalink":"/expo-audio-stream/docs/api-reference/audio-processing/extract-mel-spectrogram"}}');var s=t(4848),i=t(8453);const r={id:"audio-analysis-example",title:"Audio Analysis Example",sidebar_label:"Usage Example"},l="Audio Analysis Example",o={},d=[{value:"Complete Example",id:"complete-example",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Key API Usage",id:"key-api-usage",level:2},{value:"Recording Configuration",id:"recording-configuration",level:3},{value:"Audio Analysis Configuration",id:"audio-analysis-configuration",level:3},{value:"Performance Tips",id:"performance-tips",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"audio-analysis-example",children:"Audio Analysis Example"})}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates how to use the audio analysis features in a real-world application. We'll create a simple component that records audio, analyzes it, and displays the results."}),"\n",(0,s.jsx)(n.h2,{id:"complete-example",children:"Complete Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import React, { useState } from 'react';\nimport { View, Text, StyleSheet, Button, ScrollView } from 'react-native';\nimport { \n  useAudioRecorder, \n  extractAudioAnalysis, \n  AudioAnalysis, \n  AudioFeatures,\n  AudioRecording\n} from '@siteed/expo-audio-studio';\nimport { LineChart } from 'react-native-chart-kit';\n\ninterface FeatureDisplayProps {\n  label: string;\n  value: number | number[] | undefined;\n}\n\nconst FeatureDisplay: React.FC<FeatureDisplayProps> = ({ label, value }) => {\n  if (value === undefined) return null;\n  \n  // Format array values for display\n  const displayValue = Array.isArray(value) \n    ? value.slice(0, 3).map(v => v.toFixed(2)).join(', ') + (value.length > 3 ? '...' : '')\n    : value.toFixed(4);\n  \n  return (\n    <View style={styles.featureRow}>\n      <Text style={styles.featureLabel}>{label}:</Text>\n      <Text style={styles.featureValue}>{displayValue}</Text>\n    </View>\n  );\n};\n\nconst AudioAnalyzer: React.FC = () => {\n  const [analysis, setAnalysis] = useState<AudioAnalysis | null>(null);\n  const [isAnalyzing, setIsAnalyzing] = useState(false);\n  const [selectedFeatures, setSelectedFeatures] = useState<AudioFeatures | null>(null);\n  const [recording, setRecording] = useState<AudioRecording | null>(null);\n  \n  // Initialize the audio recorder\n  const { \n    startRecording, \n    stopRecording, \n    isRecording,\n    durationMs\n  } = useAudioRecorder();\n  \n  // Handle starting the recording\n  const handleStartRecording = async () => {\n    await startRecording({\n      sampleRate: 44100,\n      channels: 1,\n      encoding: 'pcm_16bit',\n      enableProcessing: true\n    });\n  };\n  \n  // Handle stopping the recording and analyzing the audio\n  const handleStopRecording = async () => {\n    const recordingResult = await stopRecording();\n    setRecording(recordingResult);\n    \n    if (recordingResult.fileUri) {\n      await analyzeAudio(recordingResult.fileUri);\n    }\n  };\n  \n  // Function to analyze the recorded audio\n  const analyzeAudio = async (fileUri: string) => {\n    setIsAnalyzing(true);\n    try {\n      const result = await extractAudioAnalysis({\n        fileUri,\n        segmentDurationMs: 100, // 100ms segments\n        features: {\n          energy: true,\n          rms: true,\n          zcr: true,\n          spectralCentroid: true,\n          spectralFlatness: true,\n          mfcc: true,\n          tempo: true,\n        },\n        decodingOptions: {\n          targetSampleRate: 44100,\n          targetChannels: 1,\n          targetBitDepth: 16,\n          normalizeAudio: true\n        }\n      });\n      \n      setAnalysis(result);\n      \n      // Select the first data point with features for display\n      const pointWithFeatures = result.dataPoints.find(point => point.features);\n      if (pointWithFeatures && pointWithFeatures.features) {\n        setSelectedFeatures(pointWithFeatures.features);\n      }\n    } catch (error) {\n      console.error('Error analyzing audio:', error);\n    } finally {\n      setIsAnalyzing(false);\n    }\n  };\n  \n  // Prepare data for the waveform chart\n  const chartData = analysis ? {\n    labels: [],\n    datasets: [{\n      data: analysis.dataPoints.map(point => point.amplitude),\n      color: () => 'rgba(75, 192, 192, 1)',\n      strokeWidth: 2,\n    }],\n  } : { labels: [], datasets: [{ data: [], color: () => '', strokeWidth: 0 }] };\n  \n  return (\n    <ScrollView style={styles.container}>\n      <Text style={styles.title}>Audio Analyzer</Text>\n      \n      <View style={styles.buttonContainer}>\n        {!isRecording ? (\n          <Button\n            title=\"Start Recording\"\n            onPress={handleStartRecording}\n            disabled={isAnalyzing}\n          />\n        ) : (\n          <View>\n            <Text>Recording: {(durationMs / 1000).toFixed(1)}s</Text>\n            <Button\n              title=\"Stop Recording\"\n              onPress={handleStopRecording}\n              color=\"red\"\n            />\n          </View>\n        )}\n      </View>\n      \n      {isAnalyzing && (\n        <Text style={styles.status}>Analyzing audio...</Text>\n      )}\n      \n      {analysis && (\n        <>\n          <Text style={styles.sectionTitle}>Waveform</Text>\n          <LineChart\n            data={chartData}\n            width={350}\n            height={200}\n            chartConfig={{\n              backgroundColor: '#ffffff',\n              backgroundGradientFrom: '#ffffff',\n              backgroundGradientTo: '#ffffff',\n              decimalPlaces: 2,\n              color: () => 'rgba(75, 192, 192, 1)',\n              labelColor: () => 'rgba(0, 0, 0, 1)',\n              style: {\n                borderRadius: 16,\n              },\n            }}\n            bezier\n            style={styles.chart}\n          />\n          \n          <Text style={styles.sectionTitle}>Audio Information</Text>\n          <View style={styles.infoContainer}>\n            <Text>Duration: {(analysis.durationMs / 1000).toFixed(2)} seconds</Text>\n            <Text>Sample Rate: {analysis.sampleRate} Hz</Text>\n            <Text>Bit Depth: {analysis.bitDepth} bits</Text>\n            <Text>Channels: {analysis.numberOfChannels}</Text>\n            <Text>Segments: {analysis.dataPoints.length} ({analysis.segmentDurationMs}ms each)</Text>\n          </View>\n          \n          {selectedFeatures && (\n            <>\n              <Text style={styles.sectionTitle}>Audio Features</Text>\n              <View style={styles.featuresContainer}>\n                <FeatureDisplay label=\"RMS (Loudness)\" value={selectedFeatures.rms} />\n                <FeatureDisplay label=\"Energy\" value={selectedFeatures.energy} />\n                <FeatureDisplay label=\"Zero-crossing Rate\" value={selectedFeatures.zcr} />\n                <FeatureDisplay label=\"Spectral Centroid\" value={selectedFeatures.spectralCentroid} />\n                <FeatureDisplay label=\"Spectral Flatness\" value={selectedFeatures.spectralFlatness} />\n                <FeatureDisplay label=\"Tempo (BPM)\" value={selectedFeatures.tempo} />\n                <FeatureDisplay label=\"MFCC\" value={selectedFeatures.mfcc} />\n              </View>\n            </>\n          )}\n        </>\n      )}\n    </ScrollView>\n  );\n};\n\nconst styles = StyleSheet.create({\n  container: {\n    flex: 1,\n    padding: 20,\n    backgroundColor: '#f5f5f5',\n  },\n  title: {\n    fontSize: 24,\n    fontWeight: 'bold',\n    marginBottom: 20,\n    textAlign: 'center',\n  },\n  buttonContainer: {\n    marginBottom: 20,\n    alignItems: 'center',\n  },\n  status: {\n    textAlign: 'center',\n    marginVertical: 10,\n    fontStyle: 'italic',\n  },\n  sectionTitle: {\n    fontSize: 18,\n    fontWeight: 'bold',\n    marginTop: 20,\n    marginBottom: 10,\n  },\n  chart: {\n    marginVertical: 8,\n    borderRadius: 16,\n  },\n  infoContainer: {\n    backgroundColor: '#ffffff',\n    padding: 15,\n    borderRadius: 8,\n    marginBottom: 10,\n  },\n  featuresContainer: {\n    backgroundColor: '#ffffff',\n    padding: 15,\n    borderRadius: 8,\n  },\n  featureRow: {\n    flexDirection: 'row',\n    justifyContent: 'space-between',\n    marginBottom: 8,\n  },\n  featureLabel: {\n    fontWeight: '500',\n  },\n  featureValue: {\n    fontFamily: 'monospace',\n  },\n});\n\nexport default AudioAnalyzer;\n"})}),"\n",(0,s.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Recording Setup"}),": We use the ",(0,s.jsx)(n.code,{children:"useAudioRecorder"})," hook to handle audio recording with specific configuration options."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Analysis Trigger"}),": When recording stops, we automatically analyze the audio file using ",(0,s.jsx)(n.code,{children:"extractAudioAnalysis"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Feature Extraction"}),": We specify which audio features to extract:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Basic features: RMS, energy, zero-crossing rate"}),"\n",(0,s.jsx)(n.li,{children:"Spectral features: spectral centroid, spectral flatness"}),"\n",(0,s.jsx)(n.li,{children:"Advanced features: MFCC, tempo"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Visualization"}),": We display the audio waveform using a line chart based on amplitude values."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Feature Display"}),": We show the extracted features in a readable format, handling both scalar and array values."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-api-usage",children:"Key API Usage"}),"\n",(0,s.jsx)(n.h3,{id:"recording-configuration",children:"Recording Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"await startRecording({\n  sampleRate: 44100,\n  channels: 1,\n  encoding: 'pcm_16bit',\n  enableProcessing: true\n});\n"})}),"\n",(0,s.jsx)(n.h3,{id:"audio-analysis-configuration",children:"Audio Analysis Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const result = await extractAudioAnalysis({\n  fileUri,\n  segmentDurationMs: 100, // 100ms segments\n  features: {\n    energy: true,\n    rms: true,\n    zcr: true,\n    spectralCentroid: true,\n    spectralFlatness: true,\n    mfcc: true,\n    tempo: true,\n  },\n  decodingOptions: {\n    targetSampleRate: 44100,\n    targetChannels: 1,\n    targetBitDepth: 16,\n    normalizeAudio: true\n  }\n});\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-tips",children:"Performance Tips"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"For longer recordings, consider analyzing in chunks by specifying time ranges:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"// Analyze just the first 10 seconds\nawait extractAudioAnalysis({\n  fileUri,\n  startTimeMs: 0,\n  endTimeMs: 10000,\n  // other options...\n});\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Adjust ",(0,s.jsx)(n.code,{children:"segmentDurationMs"})," based on your visualization needs:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Smaller values (e.g., 50ms) provide more detail but more data points"}),"\n",(0,s.jsx)(n.li,{children:"Larger values (e.g., 200ms) provide less detail but better performance"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"For real-time applications, focus on lightweight features (RMS, ZCR) and avoid computationally expensive ones (MFCC, tempo)"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Consider using a Web Worker for analysis on web platforms to avoid blocking the main thread"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates a basic implementation. For production applications, you might want to add error handling, loading states, and more sophisticated visualizations."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var a=t(6540);const s={},i=a.createContext(s);function r(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);