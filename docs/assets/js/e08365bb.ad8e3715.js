"use strict";(self.webpackChunkdocumentation_site=self.webpackChunkdocumentation_site||[]).push([[452],{2489:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"api-reference/audio-features/extract-audio-analysis","title":"extractAudioAnalysis","description":"The extractAudioAnalysis function is used to extract audio analysis data from a recording file. This function processes the audio data and returns an AudioAnalysis object. This information can be used to visualize audio, as demonstrated in the playground app.","source":"@site/docs/api-reference/audio-features/extract-audio-analysis.md","sourceDirName":"api-reference/audio-features","slug":"/api-reference/audio-features/extract-audio-analysis","permalink":"/expo-audio-stream/docs/api-reference/audio-features/extract-audio-analysis","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"extract-audio-analysis","title":"extractAudioAnalysis","sidebar_label":"extractAudioAnalysis"},"sidebar":"tutorialSidebar","previous":{"title":"Audio Features","permalink":"/expo-audio-stream/docs/api-reference/audio-features/audio-analysis"},"next":{"title":"Usage Example","permalink":"/expo-audio-stream/docs/api-reference/audio-features/audio-analysis-example"}}');var s=t(4848),i=t(8453);const o={id:"extract-audio-analysis",title:"extractAudioAnalysis",sidebar_label:"extractAudioAnalysis"},r="Extract Audio Analysis",l={},c=[{value:"Interface",id:"interface",level:2},{value:"Example Usage",id:"example-usage",level:2},{value:"Return Value: AudioAnalysis",id:"return-value-audioanalysis",level:2},{value:"Common Use Cases",id:"common-use-cases",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"extract-audio-analysis",children:"Extract Audio Analysis"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"extractAudioAnalysis"})," function is used to extract audio analysis data from a recording file. This function processes the audio data and returns an ",(0,s.jsx)(n.code,{children:"AudioAnalysis"})," object. This information can be used to visualize audio, as demonstrated in the playground app."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," Advanced feature extraction capabilities (spectral features, chromagram, pitch detection, etc.) are still experimental and being optimized for better performance. These features require significant processing power and may impact performance, especially during live recording scenarios."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"interface",children:"Interface"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ts",children:"// You can provide either time-based options or byte-range options\nexport type ExtractAudioAnalysisProps = TimeRangeOptions | ByteRangeOptions\n\n// Base options for all extractions\ninterface BaseExtractOptions {\n    fileUri?: string // Path to audio file\n    arrayBuffer?: ArrayBuffer // Raw audio buffer\n    segmentDurationMs?: number // Duration of each analysis segment in milliseconds (default: 100ms)\n    features?: AudioFeaturesOptions // Which audio features to extract\n    decodingOptions?: DecodingConfig // Options for decoding the audio\n    logger?: ConsoleLike // Optional logger for debugging\n}\n\n// Time-based extraction (using millisecond timestamps)\ninterface TimeRangeOptions extends BaseExtractOptions {\n    startTimeMs?: number // Start time in milliseconds\n    endTimeMs?: number // End time in milliseconds\n    position?: never // Cannot use with time-based options\n    length?: never // Cannot use with time-based options\n}\n\n// Byte-based extraction (using byte positions)\ninterface ByteRangeOptions extends BaseExtractOptions {\n    position?: number // Number of bytes to skip from the start\n    length?: number // Number of bytes to read\n    startTimeMs?: never // Cannot use with byte-based options\n    endTimeMs?: never // Cannot use with byte-based options\n}\n\n// Audio decoding configuration\ninterface DecodingConfig {\n    targetSampleRate?: number // Target sample rate for decoded audio\n    targetChannels?: number // Target number of channels\n    targetBitDepth?: BitDepth // Target bit depth (8, 16, or 32)\n    normalizeAudio?: boolean // Whether to normalize audio levels\n}\n\n// Audio features options (all are optional booleans)\ninterface AudioFeaturesOptions {\n    energy?: boolean // Extract energy feature\n    mfcc?: boolean // Extract MFCC coefficients\n    rms?: boolean // Extract RMS (loudness)\n    zcr?: boolean // Extract zero-crossing rate\n    spectralCentroid?: boolean // Extract spectral centroid\n    spectralFlatness?: boolean // Extract spectral flatness\n    spectralRolloff?: boolean // Extract spectral rolloff\n    spectralBandwidth?: boolean // Extract spectral bandwidth\n    chromagram?: boolean // Extract chromagram\n    tempo?: boolean // Extract tempo estimation\n    hnr?: boolean // Extract harmonics-to-noise ratio\n    melSpectrogram?: boolean // Extract mel spectrogram\n    spectralContrast?: boolean // Extract spectral contrast\n    tonnetz?: boolean // Extract tonnetz features\n    pitch?: boolean // Extract pitch\n    crc32?: boolean // Calculate CRC32 checksum\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsxs)(n.p,{children:["Here's an example of how to use the ",(0,s.jsx)(n.code,{children:"extractAudioAnalysis"})," function to extract audio analysis data from a file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import { extractAudioAnalysis } from '@siteed/expo-audio-studio';\n\n// Time-based extraction with specific features\nconst analysisResult = await extractAudioAnalysis({\n    fileUri: 'path/to/audio/file.wav',\n    segmentDurationMs: 100, // 100ms segments\n    startTimeMs: 1000, // Start at 1 second\n    endTimeMs: 5000, // End at 5 seconds\n    features: {\n        energy: true,\n        rms: true,\n        zcr: true,\n        spectralCentroid: true,\n        mfcc: true,\n    },\n    decodingOptions: {\n        targetSampleRate: 44100,\n        targetChannels: 1,\n        targetBitDepth: 16,\n        normalizeAudio: true,\n    },\n    logger: console,\n});\n\nconsole.log('Audio Analysis:', analysisResult);\n\n// The result contains detailed information about the audio:\nconsole.log(`Duration: ${analysisResult.durationMs / 1000} seconds`);\nconsole.log(`Sample rate: ${analysisResult.sampleRate} Hz`);\nconsole.log(`Number of data points: ${analysisResult.dataPoints.length}`);\n\n// You can access individual data points and their features\nanalysisResult.dataPoints.forEach((point, index) => {\n    if (point.features) {\n        console.log(`Point ${index} at ${point.startTime}ms:`, {\n            rms: point.features.rms,\n            energy: point.features.energy,\n            zcr: point.features.zcr,\n        });\n    }\n});\n"})}),"\n",(0,s.jsx)(n.h2,{id:"return-value-audioanalysis",children:"Return Value: AudioAnalysis"}),"\n",(0,s.jsxs)(n.p,{children:["The function returns an ",(0,s.jsx)(n.code,{children:"AudioAnalysis"})," object with the following structure:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ts",children:"interface AudioAnalysis {\n    segmentDurationMs: number // Duration of each segment in milliseconds\n    durationMs: number // Duration of the audio in milliseconds\n    bitDepth: number // Bit depth of the audio\n    samples: number // Total number of audio samples\n    numberOfChannels: number // Number of audio channels\n    sampleRate: number // Sample rate of the audio\n    dataPoints: DataPoint[] // Array of data points from the analysis\n    amplitudeRange: {\n        min: number\n        max: number\n    }\n    rmsRange: {\n        min: number\n        max: number\n    }\n    speechAnalysis?: {\n        speakerChanges: {\n            timestamp: number\n            speakerId: number\n        }[]\n    }\n}\n\ninterface DataPoint {\n    id: number\n    amplitude: number // Peak amplitude for the segment\n    rms: number // Root mean square value\n    dB: number // dBFS (decibels relative to full scale)\n    silent: boolean // Whether the segment is silent\n    features?: AudioFeatures // Extracted audio features\n    speech?: SpeechFeatures // Speech-related features\n    startTime?: number // Start time in milliseconds\n    endTime?: number // End time in milliseconds\n    startPosition?: number // Start position in bytes\n    endPosition?: number // End position in bytes\n    samples?: number // Number of audio samples in this segment\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Audio Visualization"}),": Extract amplitude and RMS values to create waveform displays."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Audio Feature Analysis"}),": Extract spectral features for audio classification or analysis."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Speech Processing"}),": Use features like MFCC for speech recognition tasks."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Music Analysis"}),": Use chromagram and tempo features for music information retrieval."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Audio Fingerprinting"}),": Use a combination of features to create unique audio fingerprints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For a complete example of how to use audio analysis in a real application, see the ",(0,s.jsx)(n.a,{href:"/expo-audio-stream/docs/api-reference/audio-features/audio-analysis-example",children:"Audio Analysis Example"})," documentation."]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var a=t(6540);const s={},i=a.createContext(s);function o(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);