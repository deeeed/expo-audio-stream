"use strict";(self.webpackChunkdocumentation_site=self.webpackChunkdocumentation_site||[]).push([[2742],{1118:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"api-reference/recording-config","title":"Recording Configuration","description":"The recording configuration specifies the settings used for audio recording on different platforms. Below are the default settings for Android, iOS, and web platforms:","source":"@site/docs/api-reference/recording-config.md","sourceDirName":"api-reference","slug":"/api-reference/recording-config","permalink":"/expo-audio-stream/docs/api-reference/recording-config","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"recording-config","title":"Recording Configuration","sidebar_label":"Recording Configuration"},"sidebar":"tutorialSidebar","previous":{"title":"API Intro","permalink":"/expo-audio-stream/docs/api-reference/api-intro"},"next":{"title":"AudioRecording","permalink":"/expo-audio-stream/docs/api-reference/audio-recording"}}');var o=i(4848),t=i(8453);const a={id:"recording-config",title:"Recording Configuration",sidebar_label:"Recording Configuration"},s=void 0,d={},c=[{value:"Platform-Specific Architecture",id:"platform-specific-architecture",level:2},{value:"Web",id:"web",level:3},{value:"Android",id:"android",level:3},{value:"iOS",id:"ios",level:3},{value:"Platform Differences",id:"platform-differences",level:2},{value:"Android and iOS",id:"android-and-ios",level:3},{value:"Web",id:"web-1",level:3},{value:"Recording Process",id:"recording-process",level:2},{value:"Example Usage",id:"example-usage",level:2},{value:"Recording Interruption Handling",id:"recording-interruption-handling",level:2},{value:"Interruption Types",id:"interruption-types",level:3},{value:"Handling Interruptions",id:"handling-interruptions",level:3},{value:"Platform Behavior",id:"platform-behavior",level:3}];function l(e){const n={blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"The recording configuration specifies the settings used for audio recording on different platforms. Below are the default settings for Android, iOS, and web platforms:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"On Android: 16kHz sample rate, 16-bit depth, 1 channel."}),"\n",(0,o.jsx)(n.li,{children:"On IOS: 48kHz sample rate, 16-bit depth, 1 channel."}),"\n",(0,o.jsx)(n.li,{children:"On the web, default configuration is 44.1kHz sample rate, 32-bit depth, 1 channel."}),"\n"]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Important Note for iOS Simulator Users"}),": When working with the iOS simulator, be aware that it has limitations regarding supported recording frequencies. For example, while real iOS devices support various sample rates (16kHz, 44.1kHz, 48kHz), the simulator may only work properly with 44.1kHz. Using unsupported frequencies in the simulator may cause the app to crash. Always test audio recording functionality on real devices for accurate behavior and support of all sample rates."]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-tsx",children:"export interface RecordingConfig {\n    sampleRate?: SampleRate // Sample rate for recording (16000, 44100, or 48000 Hz)\n    channels?: 1 | 2 // Number of audio channels (1 for mono, 2 for stereo)\n    encoding?: EncodingType // Encoding type for the recording (pcm_32bit, pcm_16bit, pcm_8bit)\n    interval?: number // Interval in milliseconds at which to emit recording data\n\n    // Device and notification settings\n    keepAwake?: boolean // Continue recording when app is in background (default is true)\n    showNotification?: boolean // Show a notification during recording (default is false)\n    showWaveformInNotification?: boolean // Show waveform in the notification (Android only)\n    notification?: NotificationConfig // Configuration for the notification\n\n    // Audio processing settings\n    enableProcessing?: boolean // Enable audio processing (default is false)\n    pointsPerSecond?: number // Number of data points to extract per second of audio (default is 10)\n    algorithm?: AmplitudeAlgorithm // Algorithm to use for amplitude computation (default is \"rms\")\n    features?: AudioFeaturesOptions // Feature options to extract (default is empty)\n\n    // Platform specific configuration\n    ios?: IOSConfig // iOS-specific configuration\n\n    // Compression settings\n    compression?: {\n        enabled: boolean\n        format: 'aac' | 'opus'  // Available compression formats\n        bitrate?: number\n    }\n\n    // Output configuration\n    outputDirectory?: string // Custom directory for saving recordings (uses app default if not specified)\n    filename?: string // Custom filename for the recording (uses UUID if not specified)\n\n    // Interruption handling\n    autoResumeAfterInterruption?: boolean // Whether to automatically resume after interruption\n    onRecordingInterrupted?: (_: RecordingInterruptionEvent) => void // Callback for interruption events\n\n    // Callback functions\n    onAudioStream?: (_: AudioDataEvent) => Promise<void> // Callback function to handle audio stream\n    onAudioAnalysis?: (_: AudioAnalysisEvent) => Promise<void> // Callback function to handle audio features\n}\n\n"})}),"\n",(0,o.jsx)(n.h2,{id:"platform-specific-architecture",children:"Platform-Specific Architecture"}),"\n",(0,o.jsx)(n.h3,{id:"web",children:"Web"}),"\n",(0,o.jsxs)(n.p,{children:["On the web, the recording utilizes the ",(0,o.jsx)(n.code,{children:"AudioWorkletProcessor"})," for handling audio data. The ",(0,o.jsx)(n.code,{children:"AudioWorkletProcessor"})," allows for real-time audio processing directly in the browser, making it a powerful tool for web-based audio applications."]}),"\n",(0,o.jsx)(n.h3,{id:"android",children:"Android"}),"\n",(0,o.jsxs)(n.p,{children:["On Android, the recording is managed using Android's native ",(0,o.jsx)(n.code,{children:"AudioRecord"})," API along with ",(0,o.jsx)(n.code,{children:"AudioFormat"})," and ",(0,o.jsx)(n.code,{children:"MediaRecorder"}),". These classes are part of the Android framework and provide low-level access to audio hardware, allowing for high-quality audio recording."]}),"\n",(0,o.jsx)(n.h3,{id:"ios",children:"iOS"}),"\n",(0,o.jsxs)(n.p,{children:["On iOS, the recording is managed using ",(0,o.jsx)(n.code,{children:"AVAudioEngine"})," and related classes from the ",(0,o.jsx)(n.code,{children:"AVFoundation"})," framework. The implementation uses a sophisticated resampling approach that:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Captures audio at the hardware's native sample rate"}),"\n",(0,o.jsx)(n.li,{children:"Performs high-quality resampling to match the requested configuration"}),"\n",(0,o.jsx)(n.li,{children:"Supports both 16-bit and 32-bit PCM formats"}),"\n",(0,o.jsx)(n.li,{children:"Maintains audio quality through intermediate Float32 format when necessary"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"platform-differences",children:"Platform Differences"}),"\n",(0,o.jsx)(n.h3,{id:"android-and-ios",children:"Android and iOS"}),"\n",(0,o.jsx)(n.p,{children:"On Android and iOS, the library attempts to record audio in the specified format. On iOS, the audio is automatically resampled to match the requested configuration using AVAudioConverter, ensuring high-quality output even when the hardware sample rate differs from the target rate."}),"\n",(0,o.jsx)(n.h3,{id:"web-1",children:"Web"}),"\n",(0,o.jsx)(n.p,{children:"On the web, the default configuration is typically higher, with a 44.1kHz sample rate and 32-bit depth. This ensures better sound quality, but it can lead to issues when resampling is required to lower settings."}),"\n",(0,o.jsx)(n.h2,{id:"recording-process",children:"Recording Process"}),"\n",(0,o.jsxs)(n.p,{children:["To start recording, you use the ",(0,o.jsx)(n.code,{children:"startRecording"})," function which accepts a ",(0,o.jsx)(n.code,{children:"RecordingConfig"})," object. The output of this function is a ",(0,o.jsx)(n.code,{children:"StartRecordingResult"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-tsx",children:"export interface StartRecordingResult {\n    fileUri: string\n    mimeType: string\n    channels?: number\n    bitDepth?: BitDepth\n    sampleRate?: SampleRate\n    compression?: {\n        compressedFileUri: string\n        size: number\n        mimeType: string\n        bitrate: number\n        format: string\n    }\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-tsx",children:"import { useAudioRecorder } from '@siteed/expo-audio-stream';\n\nconst config = {\n    sampleRate: 16000,\n    channels: 1,\n    encoding: 'pcm_16bit',\n    interval: 500,\n    enableProcessing: true,\n    keepAwake: true,\n    showNotification: true,\n    compression: {\n        enabled: true,\n        format: 'aac',\n        bitrate: 128000\n    },\n    pointsPerSecond: 1000,\n    algorithm: 'rms',\n    features: { energy: true, rms: true },\n    autoResumeAfterInterruption: true,\n    onAudioStream: async (event) => {\n        console.log('Audio data:', event);\n    },\n    onAudioAnalysis: async (data) => {\n        console.log('Processing:', data);\n    },\n    onRecordingInterrupted: (event) => {\n        console.log('Recording interrupted:', event);\n    }\n};\n\nconst {\n    startRecording,\n    stopRecording,\n    isRecording,\n    durationMs,\n    size,\n} = useAudioRecorder({ debug: true });\n\nconst handleStart = async () => {\n    const { granted } = await Audio.requestPermissionsAsync();\n    if (granted) {\n        const result = await startRecording(config);\n        console.log('Recording started with config:', result);\n    }\n};\n\nconst handleStop = async () => {\n    const result = await stopRecording();\n    console.log('Recording stopped with result:', result);\n};\n"})}),"\n",(0,o.jsx)(n.h2,{id:"recording-interruption-handling",children:"Recording Interruption Handling"}),"\n",(0,o.jsx)(n.p,{children:"The library provides robust handling of recording interruptions that may occur during audio capture. These interruptions can happen for various reasons such as incoming phone calls or audio focus changes."}),"\n",(0,o.jsx)(n.h3,{id:"interruption-types",children:"Interruption Types"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"RecordingInterruptionEvent"})," includes the following possible reasons for interruption:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-tsx",children:"type RecordingInterruptionReason =\n    | 'audioFocusLoss'    // Another app has taken audio focus\n    | 'audioFocusGain'    // Audio focus has been regained\n    | 'phoneCall'         // An incoming phone call has interrupted recording\n    | 'phoneCallEnded'    // The interrupting phone call has ended\n"})}),"\n",(0,o.jsx)(n.h3,{id:"handling-interruptions",children:"Handling Interruptions"}),"\n",(0,o.jsx)(n.p,{children:"You can handle interruptions in two ways:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Automatic Resume"}),": Set ",(0,o.jsx)(n.code,{children:"autoResumeAfterInterruption: true"})," in your config to automatically resume recording after an interruption ends."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Manual Handling"}),": Use the ",(0,o.jsx)(n.code,{children:"onRecordingInterrupted"})," callback to implement custom interruption handling:"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-tsx",children:"const config = {\n    // ... other config options ...\n    autoResumeAfterInterruption: false,\n    onRecordingInterrupted: (event) => {\n        const { reason, isPaused } = event;\n        \n        switch (reason) {\n            case 'phoneCall':\n                console.log('Recording paused due to phone call');\n                break;\n            case 'phoneCallEnded':\n                console.log('Phone call ended, can resume recording');\n                break;\n            case 'audioFocusLoss':\n                console.log('Audio focus lost to another app');\n                break;\n            case 'audioFocusGain':\n                console.log('Audio focus regained');\n                break;\n        }\n        \n        console.log('Recording is currently paused:', isPaused);\n    }\n};\n"})}),"\n",(0,o.jsx)(n.h3,{id:"platform-behavior",children:"Platform Behavior"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"iOS"}),": Interruptions are handled through the AVAudioSession system"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Android"}),": Interruptions are managed via AudioManager focus changes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Web"}),": Interruptions are handled through the Web Audio API's state changes"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var r=i(6540);const o={},t=r.createContext(o);function a(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);