"use strict";(self.webpackChunkdocumentation_site=self.webpackChunkdocumentation_site||[]).push([[2742],{1118:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"api-reference/recording-config","title":"Recording Configuration","description":"The recording configuration specifies the settings used for audio recording on different platforms. Below are the default settings for Android, iOS, and web platforms:","source":"@site/docs/api-reference/recording-config.md","sourceDirName":"api-reference","slug":"/api-reference/recording-config","permalink":"/expo-audio-stream/docs/api-reference/recording-config","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"recording-config","title":"Recording Configuration","sidebar_label":"Recording Configuration"},"sidebar":"tutorialSidebar","previous":{"title":"API Intro","permalink":"/expo-audio-stream/docs/api-reference/api-intro"},"next":{"title":"AudioRecording","permalink":"/expo-audio-stream/docs/api-reference/audio-recording"}}');var s=i(4848),o=i(8453);const t={id:"recording-config",title:"Recording Configuration",sidebar_label:"Recording Configuration"},a=void 0,d={},l=[{value:"Platform-Specific Architecture",id:"platform-specific-architecture",level:2},{value:"Web",id:"web",level:3},{value:"Android",id:"android",level:3},{value:"iOS",id:"ios",level:3},{value:"Event Emission Intervals",id:"event-emission-intervals",level:2},{value:"Performance Considerations",id:"performance-considerations",level:3},{value:"Best Practices",id:"best-practices",level:3},{value:"Platform Differences",id:"platform-differences",level:2},{value:"Android and iOS",id:"android-and-ios",level:3},{value:"Web",id:"web-1",level:3},{value:"Recording Process",id:"recording-process",level:2},{value:"Zero-Latency Recording",id:"zero-latency-recording",level:2},{value:"How it Works",id:"how-it-works",level:3},{value:"Using prepareRecording",id:"using-preparerecording",level:3},{value:"Key Benefits",id:"key-benefits",level:3},{value:"Implementation Notes",id:"implementation-notes",level:3},{value:"Example: Capture Time-Critical Audio",id:"example-capture-time-critical-audio",level:3},{value:"Web Memory Optimization",id:"web-memory-optimization",level:2},{value:"Web Configuration Options",id:"web-configuration-options",level:3},{value:"Memory Usage Control",id:"memory-usage-control",level:3},{value:"Example Usage",id:"example-usage",level:3},{value:"Platform Behavior",id:"platform-behavior",level:3},{value:"Output Configuration",id:"output-configuration",level:2},{value:"Configuration Structure",id:"configuration-structure",level:3},{value:"Supported Formats",id:"supported-formats",level:3},{value:"Usage Examples",id:"usage-examples",level:3},{value:"Accessing Output Files",id:"accessing-output-files",level:3},{value:"Platform Considerations",id:"platform-considerations",level:3},{value:"Streaming Audio Data",id:"streaming-audio-data",level:3},{value:"Buffer Duration Control",id:"buffer-duration",level:2},{value:"Configuration",id:"configuration",level:3},{value:"Performance Trade-offs",id:"performance-trade-offs",level:3},{value:"Platform Behavior",id:"platform-behavior-1",level:3},{value:"Example: Low-Latency Voice Detection",id:"example-low-latency-voice-detection",level:3},{value:"Streaming-Only Mode",id:"streaming-only",level:2},{value:"Configuration",id:"configuration-1",level:3},{value:"Benefits",id:"benefits",level:3},{value:"Important Notes",id:"important-notes",level:3},{value:"Example: Real-Time Transcription",id:"example-real-time-transcription",level:3},{value:"Example: Live Streaming to Server",id:"example-live-streaming-to-server",level:3},{value:"Combining with Buffer Duration",id:"combining-with-buffer-duration",level:3},{value:"Example Usage",id:"example-usage-1",level:2},{value:"Recording Interruption Handling",id:"recording-interruption-handling",level:2},{value:"Interruption Types",id:"interruption-types",level:3},{value:"Handling Interruptions",id:"handling-interruptions",level:3},{value:"Platform Behavior",id:"platform-behavior-2",level:3},{value:"Background Recording on iOS",id:"background-recording-on-ios",level:2},{value:"iOS Configuration",id:"ios-configuration",level:3},{value:"Troubleshooting Background Recording",id:"troubleshooting-background-recording",level:2},{value:"Example Usage",id:"example-usage-2",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"The recording configuration specifies the settings used for audio recording on different platforms. Below are the default settings for Android, iOS, and web platforms:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"On Android: 16kHz sample rate, 16-bit depth, 1 channel."}),"\n",(0,s.jsx)(n.li,{children:"On IOS: 48kHz sample rate, 16-bit depth, 1 channel."}),"\n",(0,s.jsx)(n.li,{children:"On the web, default configuration is 44.1kHz sample rate, 32-bit depth, 1 channel."}),"\n"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note on iOS Recording"}),": The library now automatically detects and adapts to the hardware's actual sample rate on both iOS devices and simulators. This means you can specify any supported sample rate (e.g., 16kHz, 44.1kHz, 48kHz) in your configuration, and the library will:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Capture audio at the hardware's native sample rate (typically 44.1kHz on simulators)"}),"\n",(0,s.jsx)(n.li,{children:"Perform high-quality resampling to match your requested sample rate"}),"\n",(0,s.jsx)(n.li,{children:"Deliver the final recording at your specified configuration"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This automatic adaptation prevents crashes that previously occurred when the requested sample rate didn't match the hardware capabilities, especially in simulators."}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"export interface RecordingConfig {\n    sampleRate?: SampleRate // Sample rate for recording (16000, 44100, or 48000 Hz)\n    channels?: 1 | 2 // Number of audio channels (1 for mono, 2 for stereo)\n    encoding?: EncodingType // Encoding type for the recording (pcm_32bit, pcm_16bit, pcm_8bit)\n    interval?: number // Interval in milliseconds at which to emit recording data (minimum: 10ms)\n    intervalAnalysis?: number // Interval in milliseconds at which to emit analysis data (minimum: 10ms)\n\n    // Device and notification settings\n    keepAwake?: boolean // Continue recording when app is in background. On iOS, requires both 'audio' and 'processing' background modes (default is true)\n    showNotification?: boolean // Show a notification during recording (default is false)\n    showWaveformInNotification?: boolean // Show waveform in the notification (Android only)\n    notification?: NotificationConfig // Configuration for the notification\n\n    // Audio processing settings\n    enableProcessing?: boolean // Enable audio processing (default is false)\n    pointsPerSecond?: number // Number of data points to extract per second of audio (default is 10)\n    algorithm?: AmplitudeAlgorithm // Algorithm to use for amplitude computation (default is \"rms\")\n    features?: AudioFeaturesOptions // Feature options to extract (default is empty)\n\n    // Platform specific configuration\n    ios?: IOSConfig // iOS-specific configuration\n    web?: WebConfig // Web-specific configuration\n\n    // Output configuration\n    output?: OutputConfig // Control which files are created during recording\n    outputDirectory?: string // Custom directory for saving recordings (uses app default if not specified)\n    filename?: string // Custom filename for the recording (uses UUID if not specified)\n\n    // Interruption handling\n    autoResumeAfterInterruption?: boolean // Whether to automatically resume after interruption\n    onRecordingInterrupted?: (_: RecordingInterruptionEvent) => void // Callback for interruption events\n\n    // Callback functions\n    onAudioStream?: (_: AudioDataEvent) => Promise<void> // Callback function to handle audio stream\n    onAudioAnalysis?: (_: AudioAnalysisEvent) => Promise<void> // Callback function to handle audio features\n    \n    // Performance options\n    bufferDurationSeconds?: number // Buffer duration in seconds (controls audio buffer size)\n}\n\n"})}),"\n",(0,s.jsx)(n.h2,{id:"platform-specific-architecture",children:"Platform-Specific Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"web",children:"Web"}),"\n",(0,s.jsxs)(n.p,{children:["On the web, the recording utilizes the ",(0,s.jsx)(n.code,{children:"AudioWorkletProcessor"})," for handling audio data. The ",(0,s.jsx)(n.code,{children:"AudioWorkletProcessor"})," allows for real-time audio processing directly in the browser, making it a powerful tool for web-based audio applications."]}),"\n",(0,s.jsx)(n.h3,{id:"android",children:"Android"}),"\n",(0,s.jsxs)(n.p,{children:["On Android, the recording is managed using Android's native ",(0,s.jsx)(n.code,{children:"AudioRecord"})," API along with ",(0,s.jsx)(n.code,{children:"AudioFormat"})," and ",(0,s.jsx)(n.code,{children:"MediaRecorder"}),". These classes are part of the Android framework and provide low-level access to audio hardware, allowing for high-quality audio recording."]}),"\n",(0,s.jsx)(n.h3,{id:"ios",children:"iOS"}),"\n",(0,s.jsxs)(n.p,{children:["On iOS, the recording is managed using ",(0,s.jsx)(n.code,{children:"AVAudioEngine"})," and related classes from the ",(0,s.jsx)(n.code,{children:"AVFoundation"})," framework. The implementation uses a sophisticated audio handling approach that:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Automatically detects and adapts to the hardware's native sample rate"}),"\n",(0,s.jsx)(n.li,{children:"Handles sample rate mismatches between iOS audio session and actual hardware capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Performs high-quality resampling to match the requested configuration"}),"\n",(0,s.jsx)(n.li,{children:"Works reliably on both physical devices and simulators regardless of the requested sample rate"}),"\n",(0,s.jsx)(n.li,{children:"Supports both 16-bit and 32-bit PCM formats"}),"\n",(0,s.jsx)(n.li,{children:"Maintains audio quality through intermediate Float32 format when necessary"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"event-emission-intervals",children:"Event Emission Intervals"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"interval"})," and ",(0,s.jsx)(n.code,{children:"intervalAnalysis"})," options control how frequently audio data and analysis events are emitted during recording. Both have a minimum value of 10ms to ensure consistent behavior across platforms while preventing excessive CPU usage."]}),"\n",(0,s.jsx)(n.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Interval"}),(0,s.jsx)(n.th,{children:"CPU Usage"}),(0,s.jsx)(n.th,{children:"Battery Impact"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"10-50ms"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Real-time visualizations, live frequency analysis"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"50-100ms"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Responsive UI updates, waveform display"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"100-500ms"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Progress indicators, level meters"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"500ms+"}),(0,s.jsx)(n.td,{children:"Very Low"}),(0,s.jsx)(n.td,{children:"Minimal"}),(0,s.jsx)(n.td,{children:"File size monitoring, duration tracking"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"For real-time visualizations"}),": Use ",(0,s.jsx)(n.code,{children:"intervalAnalysis: 10"})," with minimal features enabled"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"For general recording"}),": Use ",(0,s.jsx)(n.code,{children:"interval: 100"})," or higher to balance responsiveness and performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"For battery-sensitive apps"}),": Use intervals of 500ms or higher"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Platform considerations"}),": While both iOS and Android support 10ms intervals, actual performance may vary based on device capabilities"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Example configuration for real-time visualization:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const realtimeConfig = {\n  intervalAnalysis: 10,      // 10ms for smooth updates\n  interval: 100,             // 100ms for data emission\n  enableProcessing: true,\n  features: {\n    fft: true,              // Only enable what you need\n    energy: false,\n    rms: false\n  }\n};\n"})}),"\n",(0,s.jsx)(n.h2,{id:"platform-differences",children:"Platform Differences"}),"\n",(0,s.jsx)(n.h3,{id:"android-and-ios",children:"Android and iOS"}),"\n",(0,s.jsx)(n.p,{children:"On Android and iOS, the library attempts to record audio in the specified format. On iOS, the audio is automatically resampled to match the requested configuration using AVAudioConverter, ensuring high-quality output even when the hardware sample rate differs from the target rate."}),"\n",(0,s.jsx)(n.h3,{id:"web-1",children:"Web"}),"\n",(0,s.jsx)(n.p,{children:"On the web, the default configuration is typically higher, with a 44.1kHz sample rate and 32-bit depth. This ensures better sound quality, but it can lead to issues when resampling is required to lower settings."}),"\n",(0,s.jsx)(n.h2,{id:"recording-process",children:"Recording Process"}),"\n",(0,s.jsxs)(n.p,{children:["To start recording, you use the ",(0,s.jsx)(n.code,{children:"startRecording"})," function which accepts a ",(0,s.jsx)(n.code,{children:"RecordingConfig"})," object. The output of this function is a ",(0,s.jsx)(n.code,{children:"StartRecordingResult"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"export interface StartRecordingResult {\n    fileUri: string\n    mimeType: string\n    channels?: number\n    bitDepth?: BitDepth\n    sampleRate?: SampleRate\n    compression?: {\n        compressedFileUri: string\n        size: number\n        mimeType: string\n        bitrate: number\n        format: string\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"zero-latency-recording",children:"Zero-Latency Recording"}),"\n",(0,s.jsxs)(n.p,{children:["The library provides a ",(0,s.jsx)(n.code,{children:"prepareRecording"})," method that can significantly reduce the latency between a user action and the actual start of recording. This is particularly useful for time-sensitive applications where any delay in starting audio capture could be problematic."]}),"\n",(0,s.jsx)(n.h3,{id:"how-it-works",children:"How it Works"}),"\n",(0,s.jsxs)(n.p,{children:["When using the standard ",(0,s.jsx)(n.code,{children:"startRecording"})," function, there's an inherent delay caused by several initialization steps:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Requesting user permissions (if not already granted)"}),"\n",(0,s.jsx)(n.li,{children:"Setting up audio sessions"}),"\n",(0,s.jsx)(n.li,{children:"Allocating memory for audio buffers"}),"\n",(0,s.jsx)(n.li,{children:"Initializing hardware resources"}),"\n",(0,s.jsx)(n.li,{children:"Configuring encoders and audio processing pipelines"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"prepareRecording"})," method decouples these initialization steps from the actual recording start, allowing your application to pre-initialize all necessary resources in advance."]}),"\n",(0,s.jsx)(n.h3,{id:"using-preparerecording",children:"Using prepareRecording"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import { useAudioRecorder, useSharedAudioRecorder } from '@siteed/expo-audio-studio';\n\n// With individual recorder hook\nconst { \n  prepareRecording, \n  startRecording, \n  stopRecording \n} = useAudioRecorder();\n\n// Or with shared recorder context\nconst { \n  prepareRecording, \n  startRecording, \n  stopRecording \n} = useSharedAudioRecorder();\n\n// Prepare recording during component mounting or any appropriate initialization phase\nuseEffect(() => {\n  const prepare = async () => {\n    await prepareRecording({\n      sampleRate: 44100,\n      channels: 1,\n      encoding: 'pcm_16bit',\n      // Add any other recording configuration options\n    });\n    console.log('Recording resources prepared and ready');\n  };\n  \n  prepare();\n}, []);\n\n// Later, when the user triggers recording, it starts with minimal latency\nconst handleRecordButton = async () => {\n  await startRecording({\n    // Use the same configuration as in prepareRecording\n    sampleRate: 44100,\n    channels: 1,\n    encoding: 'pcm_16bit',\n  });\n};\n"})}),"\n",(0,s.jsx)(n.h3,{id:"key-benefits",children:"Key Benefits"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Eliminates perceptible lag"})," between user action and recording start"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Improves user experience"})," for time-sensitive applications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Consistent behavior"})," across all supported platforms"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Maintains audio quality"})," while reducing startup latency"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-notes",children:"Implementation Notes"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Call ",(0,s.jsx)(n.code,{children:"prepareRecording"})," as early as possible, such as during screen loading"]}),"\n",(0,s.jsxs)(n.li,{children:["Use identical configuration for both ",(0,s.jsx)(n.code,{children:"prepareRecording"})," and ",(0,s.jsx)(n.code,{children:"startRecording"})]}),"\n",(0,s.jsx)(n.li,{children:"The preparation state persists until recording starts or the app is terminated"}),"\n",(0,s.jsxs)(n.li,{children:["If ",(0,s.jsx)(n.code,{children:"startRecording"})," is called without prior preparation, it performs normal initialization"]}),"\n",(0,s.jsx)(n.li,{children:"Resources are automatically released when recording starts or when the component is unmounted"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-capture-time-critical-audio",children:"Example: Capture Time-Critical Audio"}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates how to implement a voice command system where capturing the beginning of speech is critical:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"function VoiceCommandScreen() {\n  const { \n    prepareRecording, \n    startRecording, \n    stopRecording,\n    isRecording \n  } = useSharedAudioRecorder();\n  \n  // Prepare audio resources when screen loads\n  useEffect(() => {\n    const prepareAudio = async () => {\n      await prepareRecording({\n        sampleRate: 16000, // Optimized for speech\n        channels: 1,\n        encoding: 'pcm_16bit',\n        enableProcessing: true,\n        features: {\n          energy: true,\n          rms: true,\n        }\n      });\n    };\n    \n    prepareAudio();\n    \n    return () => {\n      // Clean up if needed\n      if (isRecording) {\n        stopRecording();\n      }\n    };\n  }, []);\n  \n  return (\n    <View style={styles.container}>\n      <Text style={styles.instructions}>\n        Press and hold to capture voice command\n      </Text>\n      \n      <Pressable\n        onPressIn={() => startRecording({\n          sampleRate: 16000,\n          channels: 1,\n          encoding: 'pcm_16bit',\n          enableProcessing: true,\n          features: {\n            energy: true,\n            rms: true,\n          }\n        })}\n        onPressOut={stopRecording}\n        style={({ pressed }) => [\n          styles.recordButton,\n          { backgroundColor: pressed || isRecording ? 'red' : 'blue' }\n        ]}\n      >\n        <Text style={styles.buttonText}>\n          {isRecording ? 'Recording...' : 'Press to Record'}\n        </Text>\n      </Pressable>\n    </View>\n  );\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"web-memory-optimization",children:"Web Memory Optimization"}),"\n",(0,s.jsx)(n.p,{children:"On the web platform, audio recording can consume significant memory, especially for longer recordings. The library offers a memory optimization option to reduce memory usage by controlling how uncompressed audio data is stored."}),"\n",(0,s.jsx)(n.h3,{id:"web-configuration-options",children:"Web Configuration Options"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"interface WebConfig {\n    /**\n     * Whether to store uncompressed audio data for WAV generation (web only)\n     * \n     * Default: true (for backward compatibility)\n     */\n    storeUncompressedAudio?: boolean\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"memory-usage-control",children:"Memory Usage Control"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"storeUncompressedAudio"})," option lets you control how audio data is handled in memory:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"When true (default)"}),": All PCM chunks are stored in memory during recording, enabling WAV file generation when compression is disabled. This provides maximum flexibility but can use significant memory for long recordings."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"When false"}),": Only compressed audio is kept (if compression is enabled), significantly reducing memory usage. This is ideal for long recordings where memory constraints are a concern."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const { startRecording } = useAudioRecorder();\n\n// Memory-efficient recording for long sessions\nawait startRecording({\n  sampleRate: 44100,\n  channels: 1,\n  compression: {\n    enabled: true,  // Enable compression to ensure audio is captured\n    format: 'opus',\n    bitrate: 64000\n  },\n  web: {\n    storeUncompressedAudio: false  // Only store compressed data\n  }\n});\n"})}),"\n",(0,s.jsx)(n.h3,{id:"platform-behavior",children:"Platform Behavior"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Web"}),": The ",(0,s.jsx)(n.code,{children:"storeUncompressedAudio"})," setting controls in-memory storage of PCM data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"iOS/Android"}),": This setting has no effect, as these platforms always write directly to files rather than storing in memory"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"output-configuration",children:"Output Configuration"}),"\n",(0,s.jsx)(n.p,{children:"The library provides flexible control over which audio files are created during recording. You can choose to save uncompressed WAV files, compressed audio files, both, or neither (for streaming-only scenarios)."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u26a0\ufe0f Breaking Change (Web)"}),": The web-specific ",(0,s.jsx)(n.code,{children:"web.storeUncompressedAudio"})," option has been removed and replaced with ",(0,s.jsx)(n.code,{children:"output.primary.enabled"}),". See the ",(0,s.jsx)(n.a,{href:"../../../docs/BREAKING_CHANGES_OUTPUT_CONFIG.md",children:"Breaking Changes Guide"})," for migration details."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"configuration-structure",children:"Configuration Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"output?: {\n    primary?: {\n        enabled?: boolean    // Whether to create the primary WAV file (default: true)\n        format?: 'wav'       // Currently only 'wav' is supported\n    }\n    compressed?: {\n        enabled?: boolean    // Whether to create a compressed file (default: false)\n        format?: 'aac' | 'opus'  // Compression format\n        bitrate?: number     // Bitrate in bits per second (default: 128000)\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"supported-formats",children:"Supported Formats"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"AAC (Advanced Audio Coding)"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"High-quality lossy compression"}),"\n",(0,s.jsx)(n.li,{children:"Excellent for voice and music"}),"\n",(0,s.jsx)(n.li,{children:"Widely supported on all platforms"}),"\n",(0,s.jsx)(n.li,{children:"Recommended bitrate: 64000-256000 bps"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Opus"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Modern, high-efficiency codec"}),"\n",(0,s.jsx)(n.li,{children:"Superior quality at low bitrates"}),"\n",(0,s.jsx)(n.li,{children:"Excellent for speech compression"}),"\n",(0,s.jsx)(n.li,{children:"Recommended bitrate: 16000-96000 bps"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"usage-examples",children:"Usage Examples"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const { startRecording } = useAudioRecorder();\n\n// Example 1: Default behavior - only primary WAV file\nawait startRecording({\n  sampleRate: 44100,\n  channels: 1,\n  encoding: 'pcm_16bit'\n  // output is undefined, defaults to { primary: { enabled: true } }\n});\n\n// Example 2: Both WAV and compressed files\nawait startRecording({\n  sampleRate: 44100,\n  channels: 1,\n  encoding: 'pcm_16bit',\n  output: {\n    compressed: {\n      enabled: true,\n      format: 'aac',\n      bitrate: 128000 // 128 kbps\n    }\n    // primary is not specified, defaults to enabled\n  }\n});\n\n// Example 3: Only compressed file (no WAV)\nawait startRecording({\n  sampleRate: 44100,\n  channels: 1,\n  output: {\n    primary: { enabled: false },\n    compressed: {\n      enabled: true,\n      format: 'opus',\n      bitrate: 64000 // 64 kbps\n    }\n  }\n});\n\n// Example 4: Streaming only (no files)\nawait startRecording({\n  sampleRate: 16000,\n  channels: 1,\n  output: {\n    primary: { enabled: false }\n  },\n  onAudioStream: async (data) => {\n    // Process audio in real-time\n  }\n});\n"})}),"\n",(0,s.jsx)(n.h3,{id:"accessing-output-files",children:"Accessing Output Files"}),"\n",(0,s.jsx)(n.p,{children:"The recording result structure depends on which outputs were enabled:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const { stopRecording } = useAudioRecorder();\n\nconst handleStopRecording = async () => {\n  const result = await stopRecording();\n  \n  // Primary WAV file (if enabled)\n  if (result.fileUri) {\n    console.log('Primary file:', result.fileUri);\n    console.log('Primary size:', result.size, 'bytes');\n    console.log('Format:', result.mimeType); // 'audio/wav'\n  }\n  \n  // Compressed file (if enabled)\n  if (result.compression) {\n    console.log('Compressed file:', result.compression.compressedFileUri);\n    console.log('Compressed size:', result.compression.size, 'bytes');\n    console.log('Compression format:', result.compression.format);\n    console.log('Bitrate:', result.compression.bitrate, 'bps');\n  }\n  \n  // If no outputs were enabled (streaming only)\n  if (!result.fileUri && !result.compression) {\n    console.log('No files created - streaming only mode');\n  }\n};\n"})}),"\n",(0,s.jsx)(n.h3,{id:"platform-considerations",children:"Platform Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"iOS"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"AAC is supported and recommended"}),"\n",(0,s.jsxs)(n.li,{children:["Opus format is ",(0,s.jsx)(n.strong,{children:"not supported"})," on iOS - if requested, the library will automatically fall back to AAC format and emit a warning"]}),"\n",(0,s.jsx)(n.li,{children:"Files are written directly to disk rather than stored in memory"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Android"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Both AAC and Opus are supported"}),"\n",(0,s.jsx)(n.li,{children:"Files are written directly to disk rather than stored in memory"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Web"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Opus is supported"}),"\n",(0,s.jsx)(n.li,{children:"AAC support depends on browser"}),"\n",(0,s.jsxs)(n.li,{children:["Data is stored in memory during recording unless ",(0,s.jsx)(n.code,{children:"storeUncompressedAudio: false"})," is set"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"streaming-audio-data",children:"Streaming Audio Data"}),"\n",(0,s.jsxs)(n.p,{children:["You can access both raw and compressed audio data in real-time during recording using the ",(0,s.jsx)(n.code,{children:"onAudioStream"})," callback:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"await startRecording({\n  // ... other config options\n  output: {\n    compressed: {\n      enabled: true,\n      format: 'opus',\n      bitrate: 64000\n    }\n  },\n  onAudioStream: async (event) => {\n    // Raw PCM audio data (always available)\n    console.log('Raw data size:', event.eventDataSize);\n    \n    // Compressed audio chunk (if compression is enabled)\n    if (event.compression?.data) {\n      console.log('Compressed chunk size:', \n        typeof event.compression.data === 'string' \n          ? event.compression.data.length \n          : event.compression.data.size\n      );\n    }\n  }\n});\n"})}),"\n",(0,s.jsx)(n.h2,{id:"buffer-duration",children:"Buffer Duration Control"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"bufferDurationSeconds"})," option allows you to control the size of audio buffers used during recording. This affects both latency and CPU usage."]}),"\n",(0,s.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const config = {\n    bufferDurationSeconds: 0.1, // 100ms buffers\n    // ... other config options\n};\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Default Behavior"}),": When ",(0,s.jsx)(n.code,{children:"bufferDurationSeconds"})," is not specified (undefined):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The library requests 1024 frames (platform default)"}),"\n",(0,s.jsx)(n.li,{children:"At 44.1kHz, this equals ~23ms"}),"\n",(0,s.jsx)(n.li,{children:"However, iOS enforces a minimum of ~0.1s (4800 frames at 48kHz)"}),"\n",(0,s.jsx)(n.li,{children:"Android and Web respect the 1024 frame default"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-trade-offs",children:"Performance Trade-offs"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Buffer Size"}),(0,s.jsx)(n.th,{children:"Latency"}),(0,s.jsx)(n.th,{children:"CPU Usage"}),(0,s.jsx)(n.th,{children:"Best For"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"< 50ms"}),(0,s.jsx)(n.td,{children:"Very Low"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Real-time processing, voice commands"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"50-200ms"}),(0,s.jsx)(n.td,{children:"Low-Medium"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Balanced performance"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"> 200ms"}),(0,s.jsx)(n.td,{children:"Higher"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Efficient recording, battery optimization"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"platform-behavior-1",children:"Platform Behavior"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"iOS"}),": Enforces a minimum buffer size of ~0.1 seconds (4800 frames at 48kHz). Smaller requests are automatically handled through buffer accumulation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Android"}),": Respects requested buffer sizes within hardware limits"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Web"}),": Fully configurable through Web Audio API"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-low-latency-voice-detection",children:"Example: Low-Latency Voice Detection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"await startRecording({\n    sampleRate: 16000,\n    channels: 1,\n    bufferDurationSeconds: 0.02, // Request 20ms buffers\n    output: {\n        primary: { enabled: false } // No file I/O for lower latency\n    },\n    onAudioStream: async (data) => {\n        // Process voice commands with minimal delay\n        const command = await detectVoiceCommand(data);\n        if (command) {\n            await handleCommand(command);\n        }\n    }\n});\n"})}),"\n",(0,s.jsx)(n.h2,{id:"streaming-only",children:"Streaming-Only Mode"}),"\n",(0,s.jsx)(n.p,{children:"You can configure the library to stream audio data without creating any files on disk. This is ideal for real-time processing scenarios where you don't need to persist the audio."}),"\n",(0,s.jsx)(n.h3,{id:"configuration-1",children:"Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const config = {\n    output: {\n        primary: { enabled: false }  // Disable all file creation\n    },\n    // ... other config options\n};\n"})}),"\n",(0,s.jsx)(n.h3,{id:"benefits",children:"Benefits"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reduced I/O overhead"}),": No disk writes during recording"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lower storage usage"}),": No temporary files created"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Better battery life"}),": Less system resource usage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Improved performance"}),": All processing happens in memory"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"important-notes",children:"Important Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["When no outputs are enabled, the recording result will have empty ",(0,s.jsx)(n.code,{children:"fileUri"})," and no ",(0,s.jsx)(n.code,{children:"compression"})," object"]}),"\n",(0,s.jsxs)(n.li,{children:["Audio data is only available through the ",(0,s.jsx)(n.code,{children:"onAudioStream"})," callback"]}),"\n",(0,s.jsxs)(n.li,{children:["You must implement ",(0,s.jsx)(n.code,{children:"onAudioStream"})," to capture the audio data"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-real-time-transcription",children:"Example: Real-Time Transcription"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const transcriptionService = new TranscriptionService();\n\nawait startRecording({\n    sampleRate: 16000,\n    channels: 1,\n    bufferDurationSeconds: 0.05, // 50ms chunks\n    output: {\n        primary: { enabled: false }  // No files needed\n    },\n    onAudioStream: async (data) => {\n        // Send audio directly to transcription service\n        const transcript = await transcriptionService.process(data);\n        updateTranscriptUI(transcript);\n    }\n});\n\n// When stopping, no files will be returned\nconst result = await stopRecording();\nconsole.log(result.fileUri); // Will be undefined\nconsole.log(result.compression); // Will be undefined\n"})}),"\n",(0,s.jsx)(n.h3,{id:"example-live-streaming-to-server",children:"Example: Live Streaming to Server"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const websocket = new WebSocket('wss://audio-server.com/stream');\n\nawait startRecording({\n    sampleRate: 44100,\n    channels: 2,\n    bufferDurationSeconds: 0.1, // 100ms chunks for network efficiency\n    output: {\n        primary: { enabled: false }  // Stream only\n    },\n    onAudioStream: async (data) => {\n        if (websocket.readyState === WebSocket.OPEN) {\n            // Stream audio data to server\n            websocket.send(data.data);\n        }\n    }\n});\n"})}),"\n",(0,s.jsx)(n.h3,{id:"combining-with-buffer-duration",children:"Combining with Buffer Duration"}),"\n",(0,s.jsx)(n.p,{children:"These options work well together for optimizing streaming scenarios:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"// Ultra-low latency configuration\nconst lowLatencyConfig = {\n    bufferDurationSeconds: 0.01, // 10ms (will use 100ms on iOS)\n    output: {\n        primary: { enabled: false }  // No file I/O\n    },\n    // ... other options\n};\n\n// Efficient streaming configuration\nconst efficientStreamingConfig = {\n    bufferDurationSeconds: 0.2, // 200ms for network efficiency\n    output: {\n        primary: { enabled: false }  // Stream only\n    },\n    // ... other options\n};\n"})}),"\n",(0,s.jsx)(n.h2,{id:"example-usage-1",children:"Example Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import { useAudioRecorder } from '@siteed/expo-audio-studio';\n\nconst config = {\n    sampleRate: 16000,\n    channels: 1,\n    encoding: 'pcm_16bit',\n    interval: 500,\n    enableProcessing: true,\n    keepAwake: true,\n    showNotification: true,\n    output: {\n        compressed: {\n            enabled: true,\n            format: 'aac',\n            bitrate: 128000\n        }\n    },\n    pointsPerSecond: 1000,\n    algorithm: 'rms',\n    features: { energy: true, rms: true },\n    autoResumeAfterInterruption: true,\n    onAudioStream: async (event) => {\n        console.log('Audio data:', event);\n    },\n    onAudioAnalysis: async (data) => {\n        console.log('Processing:', data);\n    },\n    onRecordingInterrupted: (event) => {\n        console.log('Recording interrupted:', event);\n    }\n};\n\nconst {\n    startRecording,\n    stopRecording,\n    isRecording,\n    durationMs,\n    size,\n} = useAudioRecorder({ debug: true });\n\nconst handleStart = async () => {\n    const { granted } = await Audio.requestPermissionsAsync();\n    if (granted) {\n        const result = await startRecording(config);\n        console.log('Recording started with config:', result);\n    }\n};\n\nconst handleStop = async () => {\n    const result = await stopRecording();\n    console.log('Recording stopped with result:', result);\n};\n"})}),"\n",(0,s.jsx)(n.h2,{id:"recording-interruption-handling",children:"Recording Interruption Handling"}),"\n",(0,s.jsx)(n.p,{children:"The library provides robust handling of recording interruptions that may occur during audio capture. These interruptions can happen for various reasons such as incoming phone calls or audio focus changes."}),"\n",(0,s.jsx)(n.h3,{id:"interruption-types",children:"Interruption Types"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"RecordingInterruptionEvent"})," includes the following possible reasons for interruption:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"type RecordingInterruptionReason =\n    | 'audioFocusLoss'    // Another app has taken audio focus\n    | 'audioFocusGain'    // Audio focus has been regained\n    | 'phoneCall'         // An incoming phone call has interrupted recording\n    | 'phoneCallEnded'    // The interrupting phone call has ended\n"})}),"\n",(0,s.jsx)(n.h3,{id:"handling-interruptions",children:"Handling Interruptions"}),"\n",(0,s.jsx)(n.p,{children:"You can handle interruptions in two ways:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Automatic Resume"}),": Set ",(0,s.jsx)(n.code,{children:"autoResumeAfterInterruption: true"})," in your config to automatically resume recording after an interruption ends."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Manual Handling"}),": Use the ",(0,s.jsx)(n.code,{children:"onRecordingInterrupted"})," callback to implement custom interruption handling:"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"const config = {\n    // ... other config options ...\n    autoResumeAfterInterruption: false,\n    onRecordingInterrupted: (event) => {\n        const { reason, isPaused } = event;\n        \n        switch (reason) {\n            case 'phoneCall':\n                console.log('Recording paused due to phone call');\n                break;\n            case 'phoneCallEnded':\n                console.log('Phone call ended, can resume recording');\n                break;\n            case 'audioFocusLoss':\n                console.log('Audio focus lost to another app');\n                break;\n            case 'audioFocusGain':\n                console.log('Audio focus regained');\n                break;\n        }\n        \n        console.log('Recording is currently paused:', isPaused);\n    }\n};\n"})}),"\n",(0,s.jsx)(n.h3,{id:"platform-behavior-2",children:"Platform Behavior"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"iOS"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Interruptions are handled through the AVAudioSession system"}),"\n",(0,s.jsxs)(n.li,{children:["Phone call handling is enabled by default (can be disabled via ",(0,s.jsx)(n.code,{children:"enablePhoneStateHandling: false"}),")"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Android"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Interruptions are managed via AudioManager focus changes"}),"\n",(0,s.jsxs)(n.li,{children:["Phone call handling is enabled by default (can be disabled via ",(0,s.jsx)(n.code,{children:"enablePhoneStateHandling: false"}),")"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Web"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Interruptions are handled through the Web Audio API's state changes"}),"\n",(0,s.jsx)(n.li,{children:"Phone call handling is not supported"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"background-recording-on-ios",children:"Background Recording on iOS"}),"\n",(0,s.jsxs)(n.p,{children:["When setting ",(0,s.jsx)(n.code,{children:"keepAwake: true"})," for iOS background recording:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Required"}),": The ",(0,s.jsx)(n.code,{children:"audio"})," background mode"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This is essential for accessing the microphone in the background"}),"\n",(0,s.jsx)(n.li,{children:"Without this, iOS will suspend your audio session when backgrounded"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Recommended"}),": The ",(0,s.jsx)(n.code,{children:"processing"})," background mode"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Provides additional background execution time for audio processing"}),"\n",(0,s.jsx)(n.li,{children:"Helpful for longer recordings or when using audio analysis features"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ios-configuration",children:"iOS Configuration"}),"\n",(0,s.jsxs)(n.p,{children:["In your ",(0,s.jsx)(n.code,{children:"app.config.ts"})," or plugin configuration:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"iosBackgroundModes: {\n  useAudio: true,     // REQUIRED for background recording\n  useProcessing: true // RECOMMENDED for better performance\n}\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The key takeaway: ",(0,s.jsx)(n.code,{children:"useAudio: true"})," is required for any background recording to work."]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-background-recording",children:"Troubleshooting Background Recording"}),"\n",(0,s.jsxs)(n.p,{children:["If recording stops when your app moves to the background despite having ",(0,s.jsx)(n.code,{children:"keepAwake: true"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"iOS"}),': Verify your app has the "audio" background mode in Info.plist (required)',"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Adding the "processing" background mode is also recommended'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Android"}),": Check that your app has the required foreground service permissions"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Example iOS Info.plist configuration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"<key>UIBackgroundModes</key>\n<array>\n  <string>audio</string>\n  <string>processing</string> \x3c!-- recommended but optional --\x3e\n  \x3c!-- other background modes --\x3e\n</array>\n"})}),"\n",(0,s.jsx)(n.h2,{id:"example-usage-2",children:"Example Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import { useAudioRecorder } from '@siteed/expo-audio-studio';\n\nconst config = {\n    sampleRate: 16000,\n    channels: 1,\n    encoding: 'pcm_16bit',\n    interval: 500,\n    enableProcessing: true,\n    keepAwake: true,\n    showNotification: true,\n    output: {\n        compressed: {\n            enabled: true,\n            format: 'aac',\n            bitrate: 128000\n        }\n    },\n    pointsPerSecond: 1000,\n    algorithm: 'rms',\n    features: { energy: true, rms: true },\n    autoResumeAfterInterruption: true,\n    onAudioStream: async (event) => {\n        console.log('Audio data:', event);\n    },\n    onAudioAnalysis: async (data) => {\n        console.log('Processing:', data);\n    },\n    onRecordingInterrupted: (event) => {\n        console.log('Recording interrupted:', event);\n    }\n};\n\nconst {\n    startRecording,\n    stopRecording,\n    isRecording,\n    durationMs,\n    size,\n} = useAudioRecorder({ debug: true });\n\nconst handleStart = async () => {\n    const { granted } = await Audio.requestPermissionsAsync();\n    if (granted) {\n        const result = await startRecording(config);\n        console.log('Recording started with config:', result);\n    }\n};\n\nconst handleStop = async () => {\n    const result = await stopRecording();\n    console.log('Recording stopped with result:', result);\n};\n"})})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(6540);const s={},o=r.createContext(s);function t(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);