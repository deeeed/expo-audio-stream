"use strict";(self.webpackChunkdocumentation_site=self.webpackChunkdocumentation_site||[]).push([[5442],{6422:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"api-reference/audio-features/audio-analysis","title":"Audio Features","description":"This section describes the various audio features that can be extracted from an audio recording, including the AudioFeatures interface, AudioAnalysis, and the extractAudioAnalysis function.","source":"@site/docs/api-reference/audio-features/audio-analysis.md","sourceDirName":"api-reference/audio-features","slug":"/api-reference/audio-features/audio-analysis","permalink":"/expo-audio-stream/docs/api-reference/audio-features/audio-analysis","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"audio-analysis","title":"Audio Features","sidebar_label":"Audio Features"},"sidebar":"tutorialSidebar","previous":{"title":"Overview","permalink":"/expo-audio-stream/docs/api-reference/audio-features/audio-analysis-overview"},"next":{"title":"extractAudioAnalysis","permalink":"/expo-audio-stream/docs/api-reference/audio-features/extract-audio-analysis"}}');var s=i(4848),t=i(8453);const r={id:"audio-analysis",title:"Audio Features",sidebar_label:"Audio Features"},o="Audio Analysis and Features",l={},c=[{value:"AudioAnalysis",id:"audioanalysis",level:2},{value:"Interface",id:"interface",level:3},{value:"AudioFeatures",id:"audiofeatures",level:2},{value:"Interface",id:"interface-1",level:3},{value:"AudioFeaturesOptions",id:"audiofeaturesoptions",level:2},{value:"Interface",id:"interface-2",level:3},{value:"DataPoint",id:"datapoint",level:2},{value:"Interface",id:"interface-3",level:3},{value:"SpeechFeatures",id:"speechfeatures",level:2},{value:"Interface",id:"interface-4",level:3},{value:"Feature Descriptions",id:"feature-descriptions",level:2},{value:"Basic Features",id:"basic-features",level:3},{value:"Spectral Features",id:"spectral-features",level:3},{value:"Advanced Features",id:"advanced-features",level:3},{value:"Common Applications",id:"common-applications",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"audio-analysis-and-features",children:"Audio Analysis and Features"})}),"\n",(0,s.jsxs)(n.p,{children:["This section describes the various audio features that can be extracted from an audio recording, including the ",(0,s.jsx)(n.code,{children:"AudioFeatures"})," interface, ",(0,s.jsx)(n.code,{children:"AudioAnalysis"}),", and the ",(0,s.jsx)(n.code,{children:"extractAudioAnalysis"})," function."]}),"\n",(0,s.jsx)(n.h2,{id:"audioanalysis",children:"AudioAnalysis"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"AudioAnalysis"})," interface represents the detailed analysis of an audio signal, including the extracted audio features."]}),"\n",(0,s.jsx)(n.h3,{id:"interface",children:"Interface"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ts",children:"/**\n * Represents the complete data from the audio analysis.\n */\nexport interface AudioAnalysis {\n    segmentDurationMs: number // Duration of each segment in milliseconds\n    durationMs: number // Duration of the audio in milliseconds\n    bitDepth: number // Bit depth of the audio\n    samples: number // Total number of audio samples\n    numberOfChannels: number // Number of audio channels\n    sampleRate: number // Sample rate of the audio\n    dataPoints: DataPoint[] // Array of data points from the analysis\n    amplitudeRange: {\n        min: number\n        max: number\n    }\n    rmsRange: {\n        min: number\n        max: number\n    }\n    // Optional speech analysis data\n    speechAnalysis?: {\n        speakerChanges: {\n            timestamp: number\n            speakerId: number\n        }[]\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"audiofeatures",children:"AudioFeatures"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"AudioFeatures"})," interface represents various audio features that can be extracted from an audio signal."]}),"\n",(0,s.jsx)(n.h3,{id:"interface-1",children:"Interface"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ts",children:"/**\n * Represents various audio features extracted from an audio signal.\n */\nexport interface AudioFeatures {\n    energy?: number // The infinite integral of the squared signal, representing the overall energy of the audio.\n    mfcc?: number[] // Mel-frequency cepstral coefficients, describing the short-term power spectrum of a sound.\n    rms?: number // Root mean square value, indicating the amplitude of the audio signal.\n    minAmplitude?: number // Minimum amplitude value in the audio signal.\n    maxAmplitude?: number // Maximum amplitude value in the audio signal.\n    zcr?: number // Zero-crossing rate, indicating the rate at which the signal changes sign.\n    spectralCentroid?: number // The center of mass of the spectrum, indicating the brightness of the sound.\n    spectralFlatness?: number // Measure of the flatness of the spectrum, indicating how noise-like the signal is.\n    spectralRolloff?: number // The frequency below which a specified percentage (usually 85%) of the total spectral energy lies.\n    spectralBandwidth?: number // The width of the spectrum, indicating the range of frequencies present.\n    chromagram?: number[] // Chromagram, representing the 12 different pitch classes of the audio.\n    tempo?: number // Estimated tempo of the audio signal, measured in beats per minute (BPM).\n    hnr?: number // Harmonics-to-noise ratio, indicating the proportion of harmonics to noise in the audio signal.\n    melSpectrogram?: number[] // Mel-scaled spectrogram representation of the audio.\n    spectralContrast?: number[] // Spectral contrast features representing the difference between peaks and valleys.\n    tonnetz?: number[] // Tonal network features representing harmonic relationships.\n    pitch?: number // Pitch of the audio signal, measured in Hertz (Hz).\n    crc32?: number // crc32 checksum of the audio signal, used to verify the integrity of the audio.\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"audiofeaturesoptions",children:"AudioFeaturesOptions"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"AudioFeaturesOptions"})," interface specifies which audio features to extract during analysis."]}),"\n",(0,s.jsx)(n.h3,{id:"interface-2",children:"Interface"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ts",children:"/**\n * Options for specifying which audio features to extract.\n */\nexport interface AudioFeaturesOptions {\n    energy?: boolean\n    mfcc?: boolean\n    rms?: boolean\n    zcr?: boolean\n    spectralCentroid?: boolean\n    spectralFlatness?: boolean\n    spectralRolloff?: boolean\n    spectralBandwidth?: boolean\n    chromagram?: boolean\n    tempo?: boolean\n    hnr?: boolean\n    melSpectrogram?: boolean\n    spectralContrast?: boolean\n    tonnetz?: boolean\n    pitch?: boolean\n    crc32?: boolean\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"datapoint",children:"DataPoint"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"DataPoint"})," interface represents individual data points extracted from an audio signal during analysis."]}),"\n",(0,s.jsx)(n.h3,{id:"interface-3",children:"Interface"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ts",children:"/**\n * Represents a single data point in the audio analysis.\n */\nexport interface DataPoint {\n    id: number\n    amplitude: number // Peak amplitude for the segment\n    rms: number // Root mean square value\n    dB: number // dBFS (decibels relative to full scale) computed from RMS value\n    silent: boolean // Whether the segment is silent\n    features?: AudioFeatures // Optional extracted audio features\n    speech?: SpeechFeatures // Optional speech-related features\n    startTime?: number // Start time in milliseconds\n    endTime?: number // End time in milliseconds\n    startPosition?: number // Start position in bytes\n    endPosition?: number // End position in bytes\n    samples?: number // Number of audio samples in this segment\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"speechfeatures",children:"SpeechFeatures"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"SpeechFeatures"})," interface represents speech-related features extracted from audio."]}),"\n",(0,s.jsx)(n.h3,{id:"interface-4",children:"Interface"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ts",children:"/**\n * Represents speech-related features extracted from audio.\n */\nexport interface SpeechFeatures {\n    isActive: boolean // Whether speech is detected in this segment\n    speakerId?: number // Optional speaker identification\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"feature-descriptions",children:"Feature Descriptions"}),"\n",(0,s.jsx)(n.h3,{id:"basic-features",children:"Basic Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RMS (Root Mean Square)"}),": Measures the average power of the audio signal, correlating with perceived loudness."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Energy"}),": Represents the overall energy content of the audio segment."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Zero-Crossing Rate (ZCR)"}),": Counts how often the signal crosses the zero axis, useful for detecting voiced/unvoiced segments."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"spectral-features",children:"Spectral Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spectral Centroid"}),': Indicates the "center of mass" of the spectrum, correlating with the brightness of a sound.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spectral Flatness"}),": Measures how noise-like (versus tone-like) a sound is."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spectral Rolloff"}),": The frequency below which a specified percentage of the total spectral energy lies."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spectral Bandwidth"}),": Measures the width of the spectrum, indicating frequency range."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MFCC (Mel-Frequency Cepstral Coefficients)"}),": Compact representation of the spectral envelope, widely used in speech recognition."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Chromagram"}),": Represents the distribution of energy across the 12 pitch classes in music."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tonnetz"}),": Tonal space features representing harmonic relationships."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pitch"}),": Estimated fundamental frequency of the audio signal."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"common-applications",children:"Common Applications"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition"}),": Using MFCC, ZCR, and energy features."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Music Information Retrieval"}),": Using chromagram, tonnetz, and spectral features."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Classification"}),": Using combinations of features to identify audio types."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speaker Identification"}),": Using MFCC and other spectral features."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emotion Detection"}),": Using pitch, energy, and spectral features to detect emotional content."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For practical examples of using these features, see the ",(0,s.jsx)(n.a,{href:"/expo-audio-stream/docs/api-reference/audio-features/audio-analysis-example",children:"Audio Analysis Example"})," documentation."]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var a=i(6540);const s={},t=a.createContext(s);function r(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);